timestamp,duration(sec) / start time,machine,SRs_amount,cv_folds,models_params,accuracy,precision,recall,Comments
32:13.4,1162,de25a4c39c36,20,2,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7ff7aad9a6d8>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', in...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 0.5, 'numeric_meta_features': 0.5})), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='warn',
          tol=0.0001, verbose=0, warm_start=False))]","[0.7272727272727273, 0.6666666666666666]","[0.6666666666666666, 0.0]","[0.5, 0.0]",
56:59.6,1161,de25a4c39c36,20,2,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7f322a03e6a0>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', in...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 0.5, 'numeric_meta_features': 0.5})), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='warn',
          tol=0.0001, verbose=0, warm_start=False))]","[0.7272727272727273, 0.6666666666666666]","[0.6666666666666666, 0.0]","[0.5, 0.0]",
08:15.3,4766,de25a4c39c36,1000,5,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7f34da5fabe0>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', in...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 0.5, 'numeric_meta_features': 0.5})), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='warn',
          tol=0.0001, verbose=0, warm_start=False))]","[0.9154228855721394, 0.9054726368159204, 0.89, 0.8944723618090452, 0.8894472361809045]","[0.8857142857142857, 0.9247311827956989, 0.8504672897196262, 0.8653846153846154, 0.8787878787878788]","[0.9489795918367347, 0.8775510204081632, 0.9381443298969072, 0.9278350515463918, 0.8969072164948454]",
18:50.5,40079,de25a4c39c36,1000,2,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7f6bf80b6d68>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', in...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 0.5, 'numeric_meta_features': 0.5})), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='warn',
          tol=0.0001, verbose=0, warm_start=False))]","[0.8902195608782435, 0.9038076152304609]","[0.9126637554585153, 0.8884462151394422]","[0.8565573770491803, 0.9176954732510288]",
17:32.5,15491,de25a4c39c36,1000,2,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7fe304361c18>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', in...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 0.5, 'numeric_meta_features': 0.5})), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='warn',
          tol=0.0001, verbose=0, warm_start=False))]","[0.856, 0.896]","[0.84765625, 0.9267241379310345]","[0.868, 0.86]",
50:16.5,39224,de25a4c39c36,1000,5,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7f6e67c47c50>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', in...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 0.5, 'numeric_meta_features': 0.5})), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='warn',
          tol=0.0001, verbose=0, warm_start=False))]","[0.855, 0.845, 0.895, 0.91, 0.92]","[0.7983193277310925, 0.8, 0.8495575221238938, 0.8660714285714286, 0.8962264150943396]","[0.95, 0.92, 0.96, 0.97, 0.95]",
53:47.1,39652,de25a4c39c36,1000,5,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7f14c622bc50>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', in...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 0.5, 'numeric_meta_features': 0.5})), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='warn',
          tol=0.0001, verbose=0, warm_start=False))]","[0.5074626865671642, 0.5, 0.535, 0.505, 0.5628140703517588]","[0.5042735042735043, 0.5, 0.5271317829457365, 0.5038759689922481, 0.5588235294117647]","[0.59, 0.63, 0.68, 0.65, 0.5757575757575758]",Run with random y vector
27:36.8,39789,de25a4c39c36,1000,5,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7f03376dcc88>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', in...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 0.5, 'numeric_meta_features': 0.5})), ('clf', GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=50,
              n_iter_no_change=None, presort='auto', random_state=1984,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False))]","[0.94, 0.9, 0.9, 0.94, 0.95]","[0.9489795918367347, 0.8921568627450981, 0.8773584905660378, 0.9313725490196079, 0.9591836734693877]","[0.93, 0.91, 0.93, 0.95, 0.94]",
29:10.4,40746,de25a4c39c36,1000,5,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7fcbf69fabe0>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', in...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 0.5, 'numeric_meta_features': 0.5})), ('clf', GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=50,
              n_iter_no_change=None, presort='auto', random_state=1984,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False))]","[0.945, 0.895, 0.905, 0.945, 0.95]","[0.9587628865979382, 0.883495145631068, 0.8857142857142857, 0.9405940594059405, 0.9591836734693877]","[0.93, 0.91, 0.93, 0.95, 0.94]",
47:00.6,27733,de25a4c39c36,2487,5,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7f416a7d6b70>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', in...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 0.5, 'numeric_meta_features': 0.5})), ('clf', GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=50,
              n_iter_no_change=None, presort='auto', random_state=1984,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False))]","[0.9196787148594378, 0.9397590361445783, 0.9236947791164659, 0.9235412474849095, 0.9455645161290323]","[0.9027237354085603, 0.9357429718875502, 0.9196787148594378, 0.9446808510638298, 0.9433198380566802]","[0.9392712550607287, 0.9433198380566802, 0.9271255060728745, 0.8987854251012146, 0.9471544715447154]",
12:55.5,41:54.0,c367d41a255a,1000,5,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7fb35abc3668>), ('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', ...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 1.0, 'numeric_meta_features': 0.0})), ('clf', GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=50,
              n_iter_no_change=None, presort='auto', random_state=1984,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False))]","[0.91, 0.88, 0.915, 0.945, 0.945]","[0.9183673469387755, 0.8877551020408163, 0.9029126213592233, 0.9320388349514563, 0.9405940594059405]","[0.9, 0.87, 0.93, 0.96, 0.95]",Removing the meta-feautures (to see only lingsitc matters)
03:29.3,02:06.2,c367d41a255a,1000,5,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7fbba53fadd8>), ('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', ...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 1.0, 'numeric_meta_features': 0.0})), ('clf', GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=50,
              n_iter_no_change=None, presort='auto', random_state=1984,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False))]","[0.7860696517412935, 0.775, 0.74, 0.8, 0.7889447236180904]","[0.780952380952381, 0.7692307692307693, 0.7474747474747475, 0.7904761904761904, 0.7757009345794392]","[0.803921568627451, 0.7920792079207921, 0.7326732673267327, 0.8217821782178217, 0.8217821782178217]",Balancing between the drawing and not-drawing teams
43:05.2,21:01.6,c367d41a255a,2494,5,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7fb4932cac88>), ('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', ...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 1.0, 'numeric_meta_features': 0.0})), ('clf', GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=50,
              n_iter_no_change=None, presort='auto', random_state=1984,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False))]","[0.7835671342685371, 0.781563126252505, 0.8256513026052105, 0.8056112224448898, 0.8232931726907631]","[0.7907949790794979, 0.76953125, 0.8149606299212598, 0.8205128205128205, 0.8264462809917356]","[0.7651821862348178, 0.7975708502024291, 0.8380566801619433, 0.7773279352226721, 0.8130081300813008]",
11:22.9,32:12.8,c367d41a255a,1000,5,"[('union', FeatureUnion(n_jobs=None,
       transformer_list=[('ngram_features', Pipeline(memory=None,
     steps=[('cleanText', <clean_text_transformer.CleanTextTransformer object at 0x7fad3dd6dd68>), ('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.float64'>, encoding='utf-8', ...', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'text_features': 1.0, 'numeric_meta_features': 1.0})), ('clf', GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=50,
              n_iter_no_change=None, presort='auto', random_state=1984,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False))]","[0.815, 0.83, 0.755, 0.82, 0.795]","[0.8620689655172413, 0.8113207547169812, 0.7628865979381443, 0.7962962962962963, 0.8105263157894737]","[0.75, 0.86, 0.74, 0.86, 0.77]",Using the new SR objects - with new meta features
22:56.8,22:45.3,ABRAHAMI-MOBL1,1000,5,"[('union', FeatureUnion(n_jobs=1,
       transformer_list=[('numeric_meta_features', Pipeline(memory=None,
     steps=[('feature_extractor', <meta_feautres_extractor.MetaFeaturesExtractor object at 0x0000015A812B70B8>), ('vect', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,
        sparse=True))]))],
       transformer_weights={'numeric_meta_features': 1.0})), ('clf', GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=50,
              presort='auto', random_state=1984, subsample=1.0, verbose=0,
              warm_start=False))]","[0.78, 0.825, 0.73, 0.765, 0.735]","[0.7745098039215687, 0.8037383177570093, 0.7254901960784313, 0.7431192660550459, 0.7326732673267327]","[0.79, 0.86, 0.74, 0.81, 0.74]","Using the new SR objects - with new meta features. This run is witout the text features, only meta-features"
